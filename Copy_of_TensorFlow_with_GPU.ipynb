{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/16527/SPADE/blob/master/Copy_of_TensorFlow_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "outputId": "230c8c7a-13fd-46b3-c5b1-9c6330cfd846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y04m-jvKRDsJ",
        "outputId": "bb32a619-b9f8-4866-fe6a-40d8c5341e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.862475891000031\n",
            "GPU (s):\n",
            "0.10837535100017703\n",
            "GPU speedup over CPU: 35x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IylcXE9_C9rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "import sys, os\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "all_characters = string.printable\n",
        "number_of_characters = len(all_characters)\n",
        "\n",
        "\n",
        "def character_to_label(character):\n",
        "  \n",
        "    character_label = all_characters.find(character)\n",
        "        \n",
        "    return character_label\n",
        "\n",
        "def string_to_labels(character_string):\n",
        "    \n",
        "    #return map(lambda character: character_to_label(character), character_string)\n",
        "    return list(map(lambda character: character_to_label(character), character_string))\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        \n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "        \n",
        "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    \n",
        "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        embedded = self.encoder(input_sequences)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_sequences_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        \n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        \n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aagxew3kDGSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters))\n",
        "rnn.load_state_dict(torch.load('/content/unconditional_lyrics_rnn.pth'))\n",
        "rnn.cuda()\n",
        "\n",
        "def sample_from_rnn(starting_sting=\"Why\", sample_length=300, temperature=1):\n",
        "\n",
        "    sampled_string = starting_sting\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_sting) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = Variable(first_input)\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "    \n",
        "\n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature) ).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "\n",
        "        current_input = Variable(predicted_label.unsqueeze(1))\n",
        "\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYX2RcczEwDZ",
        "colab_type": "code",
        "outputId": "2fec16fb-fd90-4020-e62d-405b17310fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "print(sample_from_rnn(temperature=0.5, starting_sting=\"nikhil\", sample_length=500))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nikhilled in the dark  \n",
            "Oh the night is calling  \n",
            "  \n",
            "Oh the night went out and the light was strong  \n",
            "On the highways of the milky way  \n",
            "In the moonlight on the street  \n",
            "  \n",
            "Oh the king is dark and it still looks like a wonderful  \n",
            "when the night is flying  \n",
            "the stars are bright above the sea  \n",
            "  \n",
            "the times are closing in  \n",
            "playing the fire  \n",
            "the night is on the stars  \n",
            "the night is done  \n",
            "  \n",
            "innivitation  \n",
            "and the night is the dark  \n",
            "we are dancing in the night  \n",
            "we're in the street  \n",
            "  \n",
            "in the night \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAuCFT3kIa-w",
        "colab_type": "code",
        "outputId": "ded31fa6-34a6-44cf-b5c5-58d843f61a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"hello\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}